//
//  ViewController.swift
//  SmileShutter
//
//  Copyright Â© KeisukeKonishi. All rights reserved.
//

import UIKit
import AVFoundation
import CoreImage

class ViewController: UIViewController, AVCaptureVideoDataOutputSampleBufferDelegate, AVCapturePhotoCaptureDelegate {
    
    //é¡”èªè­˜
    var detector: CIDetector!
    
    //ã‚«ãƒ¡ãƒ©è¨­å®š
    var mySession: AVCaptureSession!
    var myCamera:AVCaptureDevice!
    var myVideoInput: AVCaptureDeviceInput!
    var myVideoOutput: AVCaptureVideoDataOutput!
    var myPhotoOutput: AVCapturePhotoOutput!
    
    //è¡¨ç¤º
    var faceView = UIView()
    //ã‚·ãƒ£ãƒƒã‚¿ãƒ¼ãƒ•ãƒ©ã‚°
    var shutterFlag:Bool = true
    //ãƒ•ãƒ¬ãƒ¼ãƒ ã‚«ã‚¦ãƒ³ãƒˆ
    var frameCnt = 0;
    //
    var focusPoint:CGPoint!
    //
    let speech = AVSpeechSynthesizer()
    
    override func viewDidLoad() {
        super.viewDidLoad()
        // Do any additional setup after loading the view, typically from a nib.
        
        detector = CIDetector(ofType: CIDetectorTypeFace, context: nil, options: [CIDetectorAccuracy:CIDetectorAccuracyHigh])
        
        prepareVideo()
    }
    
    func prepareVideo(){
    
        //æ¥ç¶š
        mySession = AVCaptureSession()
        mySession.sessionPreset = AVCaptureSessionPresetHigh
    
        //æ¥ç¶šå…ˆ
        myCamera = AVCaptureDevice.defaultDevice(withMediaType: AVMediaTypeVideo)
        let input = try!AVCaptureDeviceInput(device: myCamera)
        mySession.addInput(input)
    
        //ãƒ“ãƒ‡ã‚ªè¨­å®š
        myVideoOutput = AVCaptureVideoDataOutput()
        myVideoOutput.videoSettings = [kCVPixelBufferPixelFormatTypeKey as AnyHashable:Int(kCVPixelFormatType_32BGRA)]
        
        //myVideoOutput.setSampleBufferDelegate(self, queue: DispatchQueue.main)
        myVideoOutput.setSampleBufferDelegate(self, queue: DispatchQueue(label: "myqueue"))
        myVideoOutput.alwaysDiscardsLateVideoFrames = true
        mySession.addOutput(myVideoOutput)
        
        //ã‚«ãƒ¡ãƒ©è¨­å®š
        myPhotoOutput = AVCapturePhotoOutput()
        myPhotoOutput.isHighResolutionCaptureEnabled = true
        mySession.addOutput(myPhotoOutput)
    
        //ç”»åƒã‚’è¡¨ç¤ºã™ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ç”Ÿæˆ
        let myVideoLayer:AVCaptureVideoPreviewLayer = AVCaptureVideoPreviewLayer.init(session: mySession)
        myVideoLayer.frame = self.view.bounds
        myVideoLayer.videoGravity = AVLayerVideoGravityResizeAspectFill
        
        //Viewã«è¿½åŠ 
        self.view.layer.addSublayer(myVideoLayer)
        
        //ã‚«ãƒ¡ãƒ©å‘ã
        for connection in self.myVideoOutput.connections {
            if let conn = connection as? AVCaptureConnection {
                if conn.isVideoOrientationSupported {
                    conn.videoOrientation = AVCaptureVideoOrientation.portrait
                }
            }
        }
        
        //èªè­˜çµæœè¡¨ç¤ºViewã®è¨­å®š
        faceView = UIView(frame: self.view.bounds)
        self.view.addSubview(faceView)
        
        mySession.startRunning()
    }
    
    //1ãƒ•ãƒ¬ãƒ¼ãƒ æ¯ã®å‡¦ç†
    func captureOutput(_ captureOutput: AVCaptureOutput!, didOutputSampleBuffer sampleBuffer: CMSampleBuffer!, from connection: AVCaptureConnection!) {
        
        frameCnt += 1
        
        if frameCnt > 15 {
            //åŒæœŸå‡¦ç†ã§é¡”èªè­˜ã€€ï¼ˆéåŒæœŸã§ã¯ã‚­ãƒ¥ãƒ¼ãŒæºœã¾ã‚Šã™ãã‚‹ãŸã‚ï¼‰
            DispatchQueue.main.async {
                let image = self.UIImageFromCMSampleBuffer(buffer: sampleBuffer)
                self.findFace(image: image)
            }
            frameCnt = 0
        }
    }
    
    //CMSampleBufferã‚’UIImageã«å¤‰æ›ã™ã‚‹
    func UIImageFromCMSampleBuffer(buffer:CMSampleBuffer) -> UIImage{
        // ã‚µãƒ³ãƒ—ãƒ«ãƒãƒƒãƒ•ã‚¡ã‹ã‚‰ãƒ”ã‚¯ã‚»ãƒ«ãƒãƒƒãƒ•ã‚¡ã‚’å–ã‚Šå‡ºã™
        let pixelBuffer:CVImageBuffer = CMSampleBufferGetImageBuffer(buffer)!
        
        // ãƒ”ã‚¯ã‚»ãƒ«ãƒãƒƒãƒ•ã‚¡ã‚’ãƒ™ãƒ¼ã‚¹ã«CoreImageã®CIImageã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        let ciImage = CIImage(cvPixelBuffer: pixelBuffer)
        
        //CIImageã‹ã‚‰CGImageã‚’ä½œæˆ
        let pixelBufferWidth = CGFloat(CVPixelBufferGetWidth(pixelBuffer))
        let pixelBufferHeight = CGFloat(CVPixelBufferGetHeight(pixelBuffer))
        let imageRect:CGRect = CGRect(x:0,y:0,width:pixelBufferWidth, height:pixelBufferHeight)
        let ciContext = CIContext.init()
        let cgimage = ciContext.createCGImage(ciImage, from: imageRect )
        
        // CGImageã‹ã‚‰UIImageã‚’ä½œæˆ
        let image = UIImage(cgImage: cgimage!)
        return image
    }
    
    //é¡”èªè­˜å‡¦ç†
    func findFace(image:UIImage){
        guard let faceImage = CIImage(image: image) else{ return }
        
        var smileCount = 0
        var blinkCount = 0
        var faceSize:CGFloat = 0
        
        let faces = detector.features(in: faceImage, options: [CIDetectorSmile:true, CIDetectorEyeBlink:true])
        
        //æç”»ã—ãŸç”»åƒã‚’å‰Šé™¤
        for subView:UIView in self.faceView.subviews{
            subView.removeFromSuperview()
        }
        
        //æ¤œå‡ºã—ãŸé¡”æ¯ã®å‡¦ç†
        for face in faces as! [CIFaceFeature]{
            
            var faceColor : CGColor = UIColor.blue.cgColor
            
            if face.rightEyeClosed && face.leftEyeClosed{
                faceColor = UIColor.green.cgColor
                blinkCount += 1
                print("EyeClosed: ğŸ˜‘")
            }
            if face.hasSmile{
                print("ğŸ˜")
                faceColor = UIColor.red.cgColor
                smileCount += 1
            }
            
            //////////////
            //æç”»å‡¦ç†
            //////////////
            // åº§æ¨™å¤‰æ›
            var faceRect : CGRect = face.bounds
            let widthPer = (self.view.bounds.width/image.size.width)
            let heightPer = (self.view.bounds.height/image.size.height)
            
            //
            let size = widthPer * heightPer
            if faceSize < size {
                faceSize = size
                focusPoint = CGPoint(x: widthPer, y: heightPer)
            }
            // UIKitã¯å·¦ä¸Šã«åŸç‚¹ãŒã‚ã‚‹ãŒã€CoreImageã¯å·¦ä¸‹ã«åŸç‚¹ãŒã‚ã‚‹ã®ã§æƒãˆã‚‹
            faceRect.origin.y = image.size.height - faceRect.origin.y - faceRect.size.height
            
            //å€ç‡å¤‰æ›
            faceRect.origin.x = faceRect.origin.x * widthPer
            faceRect.origin.y = faceRect.origin.y * heightPer
            faceRect.size.width = faceRect.size.width * widthPer
            faceRect.size.height = faceRect.size.height * heightPer
            
            // ç”»åƒã®é¡”ã®å‘¨ã‚Šã‚’ç·šã§å›²ã†UIViewã‚’ç”Ÿæˆ.
            let faceOutline = UIView(frame: faceRect)
            faceOutline.layer.borderWidth = 1
            faceOutline.layer.borderColor = faceColor
            self.faceView.addSubview(faceOutline)
        }
        
        //æ¤œå‡ºã—ãŸé¡”ã®æ•°
        if faces.count != 0 {
            print("Number of Faces: \(faces.count)")
            print("Number of Smile: \(smileCount)")
            print("Number of Blink: \(blinkCount)")
            
            if smileCount == faces.count && blinkCount == 0 && self.shutterFlag == true{
                speech(str: "æ’®ã‚Šã¾ã™")
                print("focusPoint : \(focusPoint.x),\(focusPoint.y)")
                takePhoto()
                self.shutterFlag = false
            }
            else{
                speech(str: "ç¬‘ã£ã¦")
            }
            
            if blinkCount == faces.count && self.shutterFlag == false{
                self.shutterFlag = true
            }
        }
        //myCamera.unlockForConfiguration()
        
    }
    
    //ã‚«ãƒ¡ãƒ©æ’®å½±
    func takePhoto(){
        let settingPhoto = AVCapturePhotoSettings()
        settingPhoto.flashMode = .auto
        settingPhoto.isAutoStillImageStabilizationEnabled = true
        settingPhoto.isHighResolutionPhotoEnabled = true
        /*
         do{
         try myCamera.lockForConfiguration()
         myCamera.focusMode = .autoFocus
         myCamera.focusPointOfInterest = focusPoint
         myCamera.exposureMode = .autoExpose
         myCamera.exposurePointOfInterest = focusPoint
         //myCamera.unlockForConfiguration()
         //myPhotoOutput.capturePhoto(with: settingPhoto, delegate: self)
         }
         catch{
         
         }
         */
        myPhotoOutput.capturePhoto(with: settingPhoto, delegate: self)
    }
    
    //ã‚¢ãƒ«ãƒãƒ ã«ä¿å­˜
    func capture(_ captureOutput: AVCapturePhotoOutput, didFinishProcessingPhotoSampleBuffer photoSampleBuffer: CMSampleBuffer?, previewPhotoSampleBuffer: CMSampleBuffer?, resolvedSettings: AVCaptureResolvedPhotoSettings, bracketSettings: AVCaptureBracketedStillImageSettings?, error: Error?) {
        if let photoSampleBuffer = photoSampleBuffer{
            let photoData = AVCapturePhotoOutput.jpegPhotoDataRepresentation(forJPEGSampleBuffer: photoSampleBuffer, previewPhotoSampleBuffer: previewPhotoSampleBuffer)
            let image = UIImage(data:photoData!)
            UIImageWriteToSavedPhotosAlbum(image!, nil, nil, nil)
        }
    }
    
    //å–‹ã‚‹
    func speech(str:String){
        let utterance = AVSpeechUtterance(string: str)
        utterance.voice = AVSpeechSynthesisVoice(language: "ja-JP")
        utterance.rate = 0.4
        utterance.pitchMultiplier = 1.2
        speech.speak(utterance)
    }
    
    override func didReceiveMemoryWarning() {
        super.didReceiveMemoryWarning()
        // Dispose of any resources that can be recreated.
    }
    
    


}

